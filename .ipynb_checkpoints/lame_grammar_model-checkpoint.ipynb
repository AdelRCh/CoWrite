{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df05c91e-5712-4c85-ae38-ac8349e56211",
   "metadata": {
    "id": "df05c91e-5712-4c85-ae38-ac8349e56211"
   },
   "source": [
    "# T5-Grammar Model - local - trainable, subject to additions (grammar_model.py or whichever name it has on the repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vVew3AvJyGbC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31267,
     "status": "ok",
     "timestamp": 1686643515603,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "vVew3AvJyGbC",
    "outputId": "6a8ae0fd-b14f-46de-ae28-ae32a822e161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "RALM58sMyD7Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22568,
     "status": "ok",
     "timestamp": 1686643550948,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "RALM58sMyD7Q",
    "outputId": "86f4224b-26b8-4326-dde9-4adfc01bef31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting happytransformer\n",
      "  Downloading happytransformer-2.4.1-py3-none-any.whl (45 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.0.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.65.0)\n",
      "Collecting transformers>=4.4.0 (from happytransformer)\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=1.6.0 (from happytransformer)\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from happytransformer)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from happytransformer) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (9.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (2.27.1)\n",
      "Collecting xxhash (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (2023.4.0)\n",
      "Collecting aiohttp (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (23.1)\n",
      "Collecting responses<0.19 (from datasets>=1.6.0->happytransformer)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0->happytransformer) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0->happytransformer) (16.0.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.4.0->happytransformer) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.4.0->happytransformer)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.4.0->happytransformer)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (2.0.12)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=1.6.0->happytransformer)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=1.6.0->happytransformer)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=1.6.0->happytransformer)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=1.6.0->happytransformer)\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=1.6.0->happytransformer)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->happytransformer) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2022.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.6.0->happytransformer) (1.16.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, happytransformer\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 happytransformer-2.4.1 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.1 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install happytransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22cc986-25bf-4e79-9f7f-a11696fcd924",
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1686644944447,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "b22cc986-25bf-4e79-9f7f-a11696fcd924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 10:33:32.458616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 10:33:34.983402: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-14 10:33:38.065640: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-14 10:33:38.066294: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-14 10:33:38.066323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings, TTTrainArgs\n",
    "import gc\n",
    "\n",
    "class GrammarModel:\n",
    "    '''This class contains our grammar model.'''\n",
    "\n",
    "    def __init__(self, env_from=None):\n",
    "        '''\n",
    "        The initialization of our GrammarModel class calls an environment string from where\n",
    "        we load the model. Different environment variables will be invoked, depending on\n",
    "        env_from's value:\n",
    "\n",
    "        - 'hf' or None: HuggingFace (by default)\n",
    "        - 'gc': Google Cloud (will need to pull from environment variables)\n",
    "        - 'mlflow': form MLFlow (if applicable)\n",
    "        - 'local': On the local VM/environment\n",
    "\n",
    "        '''\n",
    "        if env_from == None or env_from == 'hf':\n",
    "            self.model = self.load_model('hf')\n",
    "        else:\n",
    "            pass # model.load from checkpoint\n",
    "\n",
    "    def train_model(self, data_source, model_path=None):\n",
    "        '''\n",
    "        This function trains our model depending on its data source.\n",
    "        At first, it will load the model. Then, it will train it on a subset of data.\n",
    "        To conclude things, the model will be saved either locally or on the cloud.\n",
    "        '''\n",
    "\n",
    "        pass # TODO: add training method here\n",
    "\n",
    "    def load_model(self, env_from='hf'):\n",
    "        '''\n",
    "        This function is supposed to load our model from somewhere.\n",
    "\n",
    "        env_from: Where we are loading our model from.\n",
    "        - 'hf': HuggingFace (by default)\n",
    "        - 'gc': Google Cloud (will need to pull from environment variables)\n",
    "        - 'mlflow': form MLFlow (if applicable)\n",
    "        - 'local': On the local VM/environment\n",
    "\n",
    "        '''\n",
    "        # For now, we're loading from HuggingFace. We'll add more stuff here IF applicable (e.g.: pretraining).\n",
    "        return HappyTextToText('TS',\"vennify/t5-base-grammar-correction\")\n",
    "\n",
    "    def check_grammar(self, input):\n",
    "        '''\n",
    "        The input of this function is a **preprocessed** string that we wish to correct.\n",
    "        The output is a string with corrections (hopefully).\n",
    "        If we don't like the output, we might want to tune the model some more.\n",
    "        '''\n",
    "        #Before making any request, we can clear any cached variables from memory.\n",
    "        gc.collect()\n",
    "\n",
    "        #Alert: preventing an error when we get an empty paragraph.\n",
    "        #It's the equivalent of the \"you KICK Miette?\" meme tweet, except for the model.\n",
    "        if not input:\n",
    "            return input\n",
    "\n",
    "        #That said, if we have something as input, we can set our settings as such:\n",
    "        beam_settings = TTSettings(num_beams=20, min_length=1, max_length=int(len(input)*(3)))\n",
    "        prelim_result = self.model.generate_text(input, args=beam_settings)\n",
    "\n",
    "        #Our output may ignore capitalization rules for the first letter within a sentence.\n",
    "        return prelim_result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c1e280-672c-4797-b163-dbae8a72e151",
   "metadata": {
    "executionInfo": {
     "elapsed": 3516,
     "status": "ok",
     "timestamp": 1686644953217,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "f9c1e280-672c-4797-b163-dbae8a72e151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GrammarModel at 0x7f179028bb80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar_model = GrammarModel(\"The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France! Its base is square, measuring 125 metres (410 ft) on each side? During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, it is the second tallest free-standing structure in France after the Millau Viaduct in Rome.\")\n",
    "grammar_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "OVM1j07RCDhz",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1686647624711,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "OVM1j07RCDhz"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "wTq_Fo-SB8oy",
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1686651576170,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "wTq_Fo-SB8oy"
   },
   "outputs": [],
   "source": [
    "new_text=re.split('[?.!]\\s', sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4844019d-0aa4-4ead-a6ab-486530ef797c",
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1686651156962,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "4844019d-0aa4-4ead-a6ab-486530ef797c"
   },
   "outputs": [],
   "source": [
    "sample_text = '''The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France! Its base is square, measuring 125 metres (410 ft) on each side? During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, it is the second tallest free-standing structure in France after the Millau Viaduct in Rome.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ea87679-5d50-41fb-8144-6393726108f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120253,
     "status": "ok",
     "timestamp": 1686658995283,
     "user": {
      "displayName": "Agata Płucienik",
      "userId": "01627576782152730907"
     },
     "user_tz": -120
    },
    "id": "1ea87679-5d50-41fb-8144-6393726108f3",
    "outputId": "aebcbf17-09dd-4888-baac-c1d3ead05a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France.\n",
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France. Its base is square, measuring 125 metres (410 feet) on each side.\n",
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France. Its base is square, measuring 125 metres (410 feet) on each side. During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930.\n",
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France. Its base is square, measuring 125 metres (410 feet) on each side. During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres.\n",
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France. Its base is square, measuring 125 metres (410 feet) on each side. During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).\n",
      "The Eiffel Tower is 324 metres tall, about the same height as an 81 storey building, and the tallest structure in France. Its base is square, measuring 125 metres (410 feet) on each side. During its construction, the tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, it is the second tallest free-standing structure in France after the Millau Viaduct in Rome.\n"
     ]
    }
   ],
   "source": [
    "new_sample_text=[]\n",
    "for sentence in new_text:\n",
    "  new_sample_text.append(grammar_model.check_grammar(sentence))\n",
    "  super_text=\" \".join(new_sample_text)\n",
    "  print(super_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
